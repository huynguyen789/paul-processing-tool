{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brainstorming:\n",
    "\n",
    "Problem: \n",
    "- User has large pdf file that has factory test data. \n",
    "- PDF file is too large: 22k pages\n",
    "- Data is in a table format\n",
    "- Data inside table is messy, need parsing/cleaning\n",
    "\n",
    "General solution:\n",
    "- Use python to parse pdf file\n",
    "- Get data from pdf file, parse data in desire format, export into excel file for visualization later\n",
    "\n",
    "Specific solution, pipeline:\n",
    "- Use camelot to parse tables from pdf files\n",
    "- Detect desire table, concat them.\n",
    "- Cleaning and organizing data into a nice/correct df:\n",
    "  - Check and drop extra column (Done)\n",
    "  - Detect and concat tables\n",
    "  - Seperate first cell into extra columns\n",
    "- Export df into excel file\n",
    "\n",
    "Design note:\n",
    "- Hardcoded: check all tables, if tables 7 columns: drop first one: all tables should have 6\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working code on small file:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import camelot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import time\n",
    "import re\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    # Read the PDF file using Camelot\n",
    "    print(f\"Start reading PDF file {file_path}...\")\n",
    "    tables = camelot.read_pdf(file_path, pages='all' )\n",
    "    # clean_tables(tables)\n",
    "    return tables\n",
    "\n",
    "def clean_table(table):\n",
    "    print(f\"\\nStart cleaning table...\")\n",
    "    # Check if the table has more than 1 column\n",
    "    if table.shape[1] > 1:\n",
    "        print(f'Table has {table.shape[1]} columns. Cleaning up...')\n",
    "        # # Drop the first column\n",
    "        table = table.drop(columns=0)\n",
    "        # Reset the column names\n",
    "        table.columns = range(table.shape[1])\n",
    "        print(f'Table now has {table.shape[1]} columns.')\n",
    "    return table\n",
    "\n",
    "# Find target_table location in the PDF file: \n",
    "# return a list of page numbers\n",
    "def find_table_location(pdf_file, target_table):\n",
    "    print(f\"Searching for table: {target_table}\")\n",
    "    \n",
    "    reader = PdfReader(pdf_file)\n",
    "\n",
    "    pages_with_table = []\n",
    "\n",
    "    for page_num in range(len(reader.pages)):\n",
    "\n",
    "        print(f\"Searching page {page_num} out of {len(reader.pages)}\")\n",
    "        \n",
    "        page = reader.pages[page_num]\n",
    "        text = page.extract_text()\n",
    "        \n",
    "        if target_table in text:\n",
    "            print(f\"Found table on page {page_num}\")\n",
    "            pages_with_table.append(page_num)\n",
    "\n",
    "            # Check for multi-page\n",
    "            if page_num < len(reader.pages)-2:\n",
    "\n",
    "                # Get page_num+2 \n",
    "                next_page = reader.pages[page_num+2]  \n",
    "                next_text = next_page.extract_text()\n",
    "\n",
    "                if re.search(r'\\d+\\.\\d+\\.\\d+', next_text):\n",
    "                    pages_with_table.append(page_num + 2)\n",
    "\n",
    "            # Add extra page    \n",
    "            if pages_with_table:\n",
    "                pages_with_table.append(pages_with_table[-1] + 2)\n",
    "\n",
    "    print(f\"Table pages found: {pages_with_table}\")\n",
    "                \n",
    "    return pages_with_table\n",
    "\n",
    "def convert_to_ranges(numbers):\n",
    "    \"\"\"\n",
    "    Convert a list of numbers into a list of ranges.\n",
    "    \"\"\"\n",
    "    ranges = []\n",
    "    for i in range(0, len(numbers), 2):\n",
    "        ranges.append(f\"{numbers[i]}-{numbers[i+1]}\")\n",
    "    return ranges\n",
    "\n",
    "def display_tables(tables):\n",
    "    # Set the max column width to a high number (e.g., 1000) to display long contents\n",
    "    pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "    # Display all tables\n",
    "    print(f\"Displaying {len(tables)} tables:\")\n",
    "    for i, table in enumerate(tables):\n",
    "        table_number = i + 1\n",
    "        print(f\"\\nTable {table_number}\")\n",
    "        display(table.df)\n",
    "\n",
    "def display_processed_tables(tables):\n",
    "    # Set the max column width to a high number (e.g., 1000) to display long contents\n",
    "    pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "    # Display all processed tables\n",
    "    print(f\"\\nDisplaying {len(tables)} tables:\")\n",
    "    for i, df in enumerate(tables):\n",
    "        print(f\"Table {i + 1}\")\n",
    "        display(df)\n",
    "\n",
    "def extract_band(tables):\n",
    "    print(f\"Extracting Band info...\")\n",
    "    for table in tables:\n",
    "        line2 = table.iloc[0,0]\n",
    "        if 'Band' in line2:\n",
    "            band_info = line2.split(' ')\n",
    "            for word in band_info:\n",
    "                if word.startswith('Band'):\n",
    "                    band_num = word[4:]  # Extract everything after \"Band\"\n",
    "                    table.insert(1, 'Band', band_num)\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"No 'Band' keyword in line: {line2}\")\n",
    "    return tables\n",
    "\n",
    "def extract_info(tables):\n",
    "    print(f\"Extracting Testname, ULCH, BW, MOD, RD info...\")\n",
    "    for i, table in enumerate(tables):\n",
    "        # Define patterns\n",
    "        testname_pattern = r\"(.*):@\"\n",
    "        ulch_pattern = r\"ULCH: (\\d+),\"\n",
    "        bw_pattern = r\"BW: ([\\d\\.]+ MHz)\"\n",
    "        mod_pattern = r\"UL_MOD_RB: ([^,]+),\"\n",
    "        rd_pattern = r\"UL_MOD_RB: [^,]+, (.*)\"\n",
    "\n",
    "        # Extract info\n",
    "        table['Testname'] = table.iloc[:,0].str.extract(testname_pattern)\n",
    "        table['ULCH'] = table.iloc[:,0].str.extract(ulch_pattern)\n",
    "        table['BW'] = table.iloc[:,0].str.extract(bw_pattern)\n",
    "        table['MOD'] = table.iloc[:,0].str.extract(mod_pattern)\n",
    "        table['RD'] = table.iloc[:,0].str.extract(rd_pattern)\n",
    "\n",
    "        # Drop the first column\n",
    "        table.drop(table.columns[0], axis=1, inplace=True)\n",
    "\n",
    "        # Reorganize the columns\n",
    "        print(f\"Reorganizing columns...\")\n",
    "        new_column_order = ['Testname', 'Band', 'ULCH', 'BW', 'MOD', 'RD', 'Limit Low', 'Limit High', 'Measured', 'Unit', 'Status']\n",
    "        table = table.reindex(columns=new_column_order)\n",
    "\n",
    "        # Replace the table in the list with the cleaned and reorganized table\n",
    "        tables[i] = table\n",
    "        \n",
    "    return tables    \n",
    "\n",
    "# Find target_table and concatenate with continuous tables \n",
    "# return a list of concatenated tables\n",
    "def find_target_table(tables, desired_name):\n",
    "    \n",
    "    current_table = None\n",
    "    desired_tables = []\n",
    "    \n",
    "    #Run through table_list\n",
    "    for table in tables:\n",
    "        first_cell = table.df.iloc[0,0].split('\\n')[0]\n",
    "\n",
    "        #If the cell in first row, first column has the desire format \"6.x.x.x\" AND the desire_name: Start a new table\n",
    "        if first_cell.startswith(\"6.\") and desired_name in first_cell:\n",
    "            # Found start of new desired table\n",
    "            print(f\"Found start of new desired table: {first_cell}\")\n",
    "            if current_table is None:\n",
    "                current_table = table\n",
    "            current_table = table.df.copy()    \n",
    "            \n",
    "        elif first_cell.startswith(\"6.\") and desired_name not in first_cell: \n",
    "            #else: return the current table, and reset the current_table to None\n",
    "            print(f\"\\nSkipping a none-desired table: {first_cell}\")\n",
    "            if current_table is not None:\n",
    "                # Save the current table\n",
    "                desired_tables.append(current_table)\n",
    "                #Reset, mark end of the desired table\n",
    "                current_table = None\n",
    "        \n",
    "        #if the table doesnt match the desire_format AND there is a current_table: concat this table into the current table:         \n",
    "        elif current_table is not None:\n",
    "            # Continuation of previous desired table\n",
    "            print(f\"Found continuation of previous desired table: {table.df.iloc[0,0]}\")\n",
    "            # print(f\"Table before clean: \")\n",
    "            # display(table.df)\n",
    "            #Clean table before concat:\n",
    "            table.df = clean_table(table.df)\n",
    "            current_table = pd.concat([current_table, table.df])\n",
    "\n",
    "            # print(f\"Table after concat:\")\n",
    "            # display(current_table)\n",
    "        \n",
    "            \n",
    "    return desired_tables\n",
    "\n",
    "# Clean, extract info in raw tables:\n",
    "# return a list of cleaned tables\n",
    "def process_tables(tables):  \n",
    "\n",
    "    processed_tables = []\n",
    "    \n",
    "    for i, table in enumerate(tables):\n",
    "        print(f\"\\nProcessing table {i+1}...\")\n",
    "        # Replace all-whitespace rows with NaN\n",
    "        table = table.replace(r'^\\s*$', np.nan, regex=True)\n",
    "        # Drop the rows where all elements are NaN\n",
    "        table = table.dropna(how='all')\n",
    "        # print(\"Table after drop empty row:\")\n",
    "        # display(table)\n",
    "    \n",
    "        # Split the first cell of the first row and use it as column headers\n",
    "        first_row = table.iloc[0, 0]\n",
    "        \n",
    "        headers = first_row.split('\\n')  \n",
    "        # print(f\"First row: {first_row}\")\n",
    "        # print(f\"Headers: {headers}\")\n",
    "        # headers.append(\"MissingHeader\")\n",
    "        table.columns = headers\n",
    "        \n",
    "        # Extract the table name from the first row\n",
    "        table_name = table.iloc[0, 0].split('\\n')[0]\n",
    "\n",
    "        # Remove the first row\n",
    "        table = table.iloc[1:]\n",
    "        \n",
    "        # Extract Band info\n",
    "        print(f\"Extracting Band info...\")\n",
    "        line2 = table.iloc[0,0]\n",
    "        # print(f\"line2: {line2}\")\n",
    "        if 'Band' in line2:\n",
    "            band_info = line2.split(' ')\n",
    "            for word in band_info:\n",
    "                if word.startswith('Band'):\n",
    "                    band_num = word[4:]  # Extract everything after \"Band\"\n",
    "                    table.insert(1, 'Band', band_num)\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"No 'Band' keyword in line: {line2}\")\n",
    "        # Drop the first row\n",
    "        table = table.drop(table.index[0])\n",
    "        # print(\"Table after header:\")\n",
    "        # display(table)\n",
    "          \n",
    "        # Check if the expected columns are in the table\n",
    "        if 'Unit' not in table.columns:\n",
    "            print(f\"Table {i+1} doesn't have the expected structure. Skipping...\")\n",
    "            continue\n",
    "  \n",
    "        # Extract Testname, ULCH, BW, MOD, RD info\n",
    "        print(f\"Extracting Testname, ULCH, BW, MOD, RD info...\")\n",
    "        # Define patterns\n",
    "        testname_pattern = r\"(.*):@\"\n",
    "        ulch_pattern = r\"ULCH: (\\d+),\"\n",
    "        bw_pattern = r\"BW: ([\\d\\.]+ MHz)\"\n",
    "        mod_pattern = r\"UL_MOD_RB: ([^,]+),\"\n",
    "        rd_pattern = r\"UL_MOD_RB: [^,]+, (.*)\"\n",
    "\n",
    "        # Extract info\n",
    "        table['Testname'] = table.iloc[:,0].str.extract(testname_pattern)\n",
    "        table['ULCH'] = table.iloc[:,0].str.extract(ulch_pattern)\n",
    "        table['BW'] = table.iloc[:,0].str.extract(bw_pattern)\n",
    "        table['MOD'] = table.iloc[:,0].str.extract(mod_pattern)\n",
    "        table['RD'] = table.iloc[:,0].str.extract(rd_pattern)\n",
    "\n",
    "        # Create a temporary DataFrame for the split results\n",
    "        split_df = table['Unit'].str.split(expand=True)\n",
    "\n",
    "        # Assign the split results to 'Measured' and 'Unit' only where there are values\n",
    "        table.loc[split_df[0].notna(), 'Measured'] = split_df.loc[split_df[0].notna(), 0]\n",
    "        table.loc[split_df[1].notna(), 'Unit'] = split_df.loc[split_df[1].notna(), 1]\n",
    "\n",
    "        # Drop the first column\n",
    "        table.drop(table.columns[0], axis=1, inplace=True)\n",
    "\n",
    "        # Create a new column filled with the table name\n",
    "        table.insert(0, 'Table Name', table_name)\n",
    "        table = table.iloc[1:]\n",
    "        \n",
    "        # Reorganize the columns\n",
    "        print(f\"Reorganizing columns...\")\n",
    "        new_column_order = ['Table Name', 'Testname', 'Band', 'ULCH', 'BW', 'MOD', 'RD', 'Limit Low', 'Limit High', 'Measured', 'Unit', 'Status']\n",
    "        table = table.reindex(columns=new_column_order)\n",
    "\n",
    "        # Reset the index\n",
    "        table.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        #Append processed table to processed_tables:\n",
    "        processed_tables.append(table)\n",
    "        \n",
    "        \n",
    "    return processed_tables\n",
    "\n",
    "\n",
    "short_file = \"2_pages_test.pdf\"\n",
    "\n",
    "# \"LTE_3GPP_v15_r8_FDD_FORD_All_TEMPS_TCU2_5_ROW_012023_5GSIM_AT_2023-06-28_16-27-57_188.pdf\"\n",
    "#  \"25C_DATA_Extract(1temperatureOnly).pdf\"\n",
    "# \"300_pages_extract.pdf\"\n",
    "long_file = \"LTE_3GPP_v15_r8_FDD_FORD_All_TEMPS_TCU2_5_ROW_012023_5GSIM_AT_2023-06-28_16-27-57_188.pdf\"\n",
    "target_table = \"6.2.2 Maximum Output Power\" \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run on large file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find target table pages\n",
    "pages = find_table_location(long_file, target_table)\n",
    "\n",
    "# Format page numbers for camelot\n",
    "page_ranges = convert_to_ranges(pages)\n",
    "print(f\"Pages to read: {page_ranges}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages to read: ['3-5', '291-293', '815-817', '1212-1214', '1736-1738', '2072-2074', '2358-2360', '2694-2696', '2980-2982', '3375-3377', '3660-3662', '3945-3947', '4437-4439', '4726-4728', '5250-5252', '5647-5649', '6171-6173', '6507-6509', '6793-6795', '7129-7131', '7415-7417', '7810-7812', '8095-8097', '8380-8382', '8872-8874', '9161-9163', '9685-9687', '10082-10084', '10606-10608', '10942-10944', '11228-11230', '11564-11566', '11850-11852', '12245-12247', '12530-12532', '12815-12817', '13307-13309', '13596-13598', '14120-14122', '14517-14519', '15041-15043', '15377-15379', '15663-15665', '15999-16001', '16285-16287', '16680-16682', '16965-16967', '17250-17252', '17742-17744', '18030-18032', '18554-18556', '18951-18953', '19475-19477', '19811-19813', '20097-20099', '20433-20435', '20719-20721', '21114-21116', '21399-21401', '21684-21686']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Pages to read: {page_ranges}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tables = []\n",
    "   \n",
    "for page_range in page_ranges:\n",
    "    print(f\"\\nProcessing page range: {page_range}\")\n",
    " \n",
    "    # Read all tables from pdf\n",
    "    long_tables = camelot.read_pdf(long_file, pages=page_range)\n",
    "\n",
    "    desire_tables = find_target_table(long_tables, \"6.2.2 Maximum Output Power\")\n",
    "    # display_processed_tables(desire_tables)\n",
    "\n",
    "    processed_tables = process_tables(desire_tables)\n",
    "    #append to clean_tables:\n",
    "    clean_tables.extend(processed_tables)\n",
    "\n",
    "\n",
    "display_processed_tables(clean_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all the dataframes in the list into a single dataframe\n",
    "all_tables = pd.concat(clean_tables, ignore_index=True)\n",
    "all_tables.head(10) # Display the first few rows of the resulting dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"22kPDF.txt\", \"r\") as file:\n",
    "    page_ranges = file.read().splitlines()\n",
    "page_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing page range 1/60...\n",
      "Estimated time remaining: 9.00 minutes\n"
     ]
    },
    {
     "ename": "GhostscriptError",
     "evalue": "-100",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mGhostscriptError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 160\u001b[0m\n\u001b[1;32m    158\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    159\u001b[0m \u001b[39m# Read all tables from pdf\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m long_tables \u001b[39m=\u001b[39m camelot\u001b[39m.\u001b[39;49mread_pdf(long_file, pages\u001b[39m=\u001b[39;49mpage_range)\n\u001b[1;32m    161\u001b[0m end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    162\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTime taken to load table: \u001b[39m\u001b[39m{\u001b[39;00mend_time\u001b[39m \u001b[39m\u001b[39m-\u001b[39m\u001b[39m \u001b[39mstart_time\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m seconds\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/paul/lib/python3.11/site-packages/camelot/io.py:113\u001b[0m, in \u001b[0;36mread_pdf\u001b[0;34m(filepath, pages, password, flavor, suppress_stdout, layout_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m p \u001b[39m=\u001b[39m PDFHandler(filepath, pages\u001b[39m=\u001b[39mpages, password\u001b[39m=\u001b[39mpassword)\n\u001b[1;32m    112\u001b[0m kwargs \u001b[39m=\u001b[39m remove_extra(kwargs, flavor\u001b[39m=\u001b[39mflavor)\n\u001b[0;32m--> 113\u001b[0m tables \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39;49mparse(\n\u001b[1;32m    114\u001b[0m     flavor\u001b[39m=\u001b[39;49mflavor,\n\u001b[1;32m    115\u001b[0m     suppress_stdout\u001b[39m=\u001b[39;49msuppress_stdout,\n\u001b[1;32m    116\u001b[0m     layout_kwargs\u001b[39m=\u001b[39;49mlayout_kwargs,\n\u001b[1;32m    117\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    118\u001b[0m )\n\u001b[1;32m    119\u001b[0m \u001b[39mreturn\u001b[39;00m tables\n",
      "File \u001b[0;32m~/anaconda3/envs/paul/lib/python3.11/site-packages/camelot/handlers.py:173\u001b[0m, in \u001b[0;36mPDFHandler.parse\u001b[0;34m(self, flavor, suppress_stdout, layout_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m     parser \u001b[39m=\u001b[39m Lattice(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs) \u001b[39mif\u001b[39;00m flavor \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlattice\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m Stream(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    172\u001b[0m     \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m pages:\n\u001b[0;32m--> 173\u001b[0m         t \u001b[39m=\u001b[39m parser\u001b[39m.\u001b[39;49mextract_tables(\n\u001b[1;32m    174\u001b[0m             p, suppress_stdout\u001b[39m=\u001b[39;49msuppress_stdout, layout_kwargs\u001b[39m=\u001b[39;49mlayout_kwargs\n\u001b[1;32m    175\u001b[0m         )\n\u001b[1;32m    176\u001b[0m         tables\u001b[39m.\u001b[39mextend(t)\n\u001b[1;32m    177\u001b[0m \u001b[39mreturn\u001b[39;00m TableList(\u001b[39msorted\u001b[39m(tables))\n",
      "File \u001b[0;32m~/anaconda3/envs/paul/lib/python3.11/site-packages/camelot/parsers/lattice.py:402\u001b[0m, in \u001b[0;36mLattice.extract_tables\u001b[0;34m(self, filename, suppress_stdout, layout_kwargs)\u001b[0m\n\u001b[1;32m    397\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    398\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mNo tables found on \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mbasename(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrootname))\n\u001b[1;32m    399\u001b[0m         )\n\u001b[1;32m    400\u001b[0m     \u001b[39mreturn\u001b[39;00m []\n\u001b[0;32m--> 402\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate_image()\n\u001b[1;32m    403\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_generate_table_bbox()\n\u001b[1;32m    405\u001b[0m _tables \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m~/anaconda3/envs/paul/lib/python3.11/site-packages/camelot/parsers/lattice.py:219\u001b[0m, in \u001b[0;36mLattice._generate_image\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    217\u001b[0m gs_call \u001b[39m=\u001b[39m gs_call\u001b[39m.\u001b[39mencode()\u001b[39m.\u001b[39msplit()\n\u001b[1;32m    218\u001b[0m null \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(os\u001b[39m.\u001b[39mdevnull, \u001b[39m\"\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 219\u001b[0m \u001b[39mwith\u001b[39;00m Ghostscript(\u001b[39m*\u001b[39;49mgs_call, stdout\u001b[39m=\u001b[39;49mnull) \u001b[39mas\u001b[39;00m gs:\n\u001b[1;32m    220\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    221\u001b[0m null\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/anaconda3/envs/paul/lib/python3.11/site-packages/camelot/ext/ghostscript/__init__.py:88\u001b[0m, in \u001b[0;36mGhostscript\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39m# Ghostscript only supports a single instance\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[39mif\u001b[39;00m __instance__ \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 88\u001b[0m     __instance__ \u001b[39m=\u001b[39m gs\u001b[39m.\u001b[39;49mnew_instance()\n\u001b[1;32m     89\u001b[0m \u001b[39mreturn\u001b[39;00m __Ghostscript(\n\u001b[1;32m     90\u001b[0m     __instance__,\n\u001b[1;32m     91\u001b[0m     args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m     stderr\u001b[39m=\u001b[39mkwargs\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mstderr\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m),\n\u001b[1;32m     95\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/paul/lib/python3.11/site-packages/camelot/ext/ghostscript/_gsprint.py:71\u001b[0m, in \u001b[0;36mnew_instance\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m rc \u001b[39m=\u001b[39m libgs\u001b[39m.\u001b[39mgsapi_new_instance(pointer(instance), display_callback)\n\u001b[1;32m     70\u001b[0m \u001b[39mif\u001b[39;00m rc \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m---> 71\u001b[0m     \u001b[39mraise\u001b[39;00m GhostscriptError(rc)\n\u001b[1;32m     72\u001b[0m \u001b[39mreturn\u001b[39;00m instance\n",
      "\u001b[0;31mGhostscriptError\u001b[0m: -100"
     ]
    }
   ],
   "source": [
    "# Find target_table and concatenate with continuous tables \n",
    "# return a list of concatenated tables\n",
    "def find_target_table(tables, desired_name):\n",
    "    \n",
    "    current_table = None\n",
    "    desired_tables = []\n",
    "    \n",
    "    #Run through table_list\n",
    "    for table in tables:\n",
    "        first_cell = table.df.iloc[0,0].split('\\n')[0]\n",
    "\n",
    "        #If the cell in first row, first column has the desire format \"6.x.x.x\" AND the desire_name: Start a new table\n",
    "        if first_cell.startswith(\"6.\") and desired_name in first_cell:\n",
    "            # Found start of new desired table\n",
    "            \n",
    "            if current_table is None:\n",
    "                current_table = table\n",
    "            current_table = table.df.copy()\n",
    "            # print(f\"Found start of new desired table:\")   \n",
    "            # display(current_table)\n",
    "        elif first_cell.startswith(\"6.\") and desired_name not in first_cell: \n",
    "            #else: return the current table, and reset the current_table to None\n",
    "            print(f\"\\nSkipping a none-desired table: {first_cell}\")\n",
    "            if current_table is not None:\n",
    "                # Save the current table\n",
    "                desired_tables.append(current_table)\n",
    "                #Reset, mark end of the desired table\n",
    "                current_table = None\n",
    "        \n",
    "        #if the table doesnt match the desire_format AND there is a current_table: concat this table into the current table:         \n",
    "        elif current_table is not None:\n",
    "            # Continuation of previous desired table\n",
    "            # print(f\"Found continuation of previous desired table: {table.df.iloc[0,0]}\")\n",
    "            # print(f\"Found continuous table, before cleaning:\")   \n",
    "            # display(table.df)\n",
    "            #Clean table before concat:\n",
    "            table.df = clean_table(table.df)\n",
    "            # print(f\"Continuous table after cleaning:\")   \n",
    "            # display(table.df)\n",
    "            current_table = pd.concat([current_table, table.df])\n",
    "\n",
    "            # print(f\"Table after concat:\")\n",
    "            # display(current_table)\n",
    "        \n",
    "            \n",
    "    return desired_tables\n",
    "\n",
    "# Clean, extract info in raw tables:\n",
    "# return a list of cleaned tables\n",
    "def process_tables(tables):  \n",
    "\n",
    "    processed_tables = []\n",
    "    \n",
    "    for i, table in enumerate(tables):\n",
    "        print(f\"\\nProcessing table {i+1}...\")\n",
    "        print(f\"Table before processing:\")\n",
    "        display(table)\n",
    "\n",
    "    \n",
    "        # Split the first cell of the first row and use it as column headers\n",
    "        first_row = table.iloc[0, 0]\n",
    "        headers = first_row.split('\\n')  \n",
    "        # print(f\"First row: {first_row}\")\n",
    "        # print(f\"Headers: {headers}\")\n",
    "        # headers.append(\"MissingHeader\")\n",
    "        table.columns = headers\n",
    "        \n",
    "        # Extract the table name from the first row\n",
    "        table_name = table.iloc[0, 0].split('\\n')[0]\n",
    "\n",
    "        \n",
    "        # Extract Band info from column 2\n",
    "        print(f\"Extracting Band info...\")\n",
    "        line2 = table.iloc[1,0]\n",
    "        print(f\"line2: {line2}\")\n",
    "        if 'Band' in line2:\n",
    "            band_info = line2.split(' ')\n",
    "            for word in band_info:\n",
    "                if word.startswith('Band'):\n",
    "                    band_num = word[4:]  # Extract everything after \"Band\"\n",
    "                    table.insert(1, 'Band', band_num)\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"No 'Band' keyword in line: {line2}\")\n",
    "         \n",
    "\n",
    "        # print(\"Table after header:\")\n",
    "        # display(table)\n",
    "          \n",
    "        # # Check if the expected columns are in the table\n",
    "        # if 'Unit' not in table.columns:\n",
    "        #     print(f\"Table {i+1} doesn't have the expected structure. Skipping...\")\n",
    "        #     continue\n",
    "  \n",
    "        # Extract Testname, ULCH, BW, MOD, RD info\n",
    "        print(f\"Extracting Testname, ULCH, BW, MOD, RD info...\")\n",
    "        # Define patterns\n",
    "        testname_pattern = r\"(.*):@\"\n",
    "        ulch_pattern = r\"ULCH: (\\d+),\"\n",
    "        bw_pattern = r\"BW: ([\\d\\.]+ MHz)\"\n",
    "        mod_pattern = r\"UL_MOD_RB: ([^,]+),\"\n",
    "        rd_pattern = r\"UL_MOD_RB: [^,]+, (.*)\"\n",
    "\n",
    "        # Extract info\n",
    "        table['Testname'] = table.iloc[:,0].str.extract(testname_pattern)\n",
    "        table['ULCH'] = table.iloc[:,0].str.extract(ulch_pattern)\n",
    "        table['BW'] = table.iloc[:,0].str.extract(bw_pattern)\n",
    "        table['MOD'] = table.iloc[:,0].str.extract(mod_pattern)\n",
    "        table['RD'] = table.iloc[:,0].str.extract(rd_pattern)\n",
    "        \n",
    "        # print(f\"Table before split Unit column:\")\n",
    "        # display(table)\n",
    "   \n",
    "        # Split 'Measured' and 'Unit' columns \n",
    "        # Create a temporary DataFrame for the split results\n",
    "        split_df = table['Unit'].str.split(expand=True)\n",
    "\n",
    "        # Assign the split results to 'Measured' and 'Unit' only where there are values\n",
    "        table.loc[split_df[0].notna(), 'Measured'] = split_df.loc[split_df[0].notna(), 0]\n",
    "        table.loc[split_df[1].notna(), 'Unit'] = split_df.loc[split_df[1].notna(), 1]\n",
    "\n",
    "        # Drop the first column\n",
    "        table.drop(table.columns[0], axis=1, inplace=True)\n",
    "\n",
    "        # Create a new column filled with the table name\n",
    "        table.insert(0, 'Table Name', table_name)\n",
    "        table = table.iloc[1:]\n",
    "        \n",
    "        # Reorganize the columns\n",
    "        print(f\"Reorganizing columns...\")\n",
    "        new_column_order = ['Table Name', 'Testname', 'Band', 'ULCH', 'BW', 'MOD', 'RD', 'Limit Low', 'Limit High', 'Measured', 'Unit', 'Status']\n",
    "        table = table.reindex(columns=new_column_order)\n",
    "\n",
    "        # Drop rows with NaN values in Testname and Band columns\n",
    "        table = table.dropna(subset=['Testname'], how='all')\n",
    "\n",
    "\n",
    "        # Reset the index\n",
    "        table.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        #Append processed table to processed_tables:\n",
    "        processed_tables.append(table)\n",
    "        \n",
    "        \n",
    "    return processed_tables\n",
    "\n",
    "\n",
    "clean_tables = []\n",
    "total_ranges = len(page_ranges)\n",
    "for i, page_range in enumerate(page_ranges):\n",
    "    \n",
    "    print(f\"Processing page range {i+1}/{total_ranges}...\")\n",
    "    # Calculate the estimate time remaining based on the progress so far\n",
    "    progress = i / total_ranges\n",
    "    time_remaining = (total_ranges - i) * 9 / 60\n",
    "    print(f\"Estimated time remaining: {time_remaining:.2f} minutes\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    # Read all tables from pdf\n",
    "    long_tables = camelot.read_pdf(long_file, pages=page_range)\n",
    "    end_time = time.time()\n",
    "    print(f\"Time taken to load table: {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    desire_tables = find_target_table(long_tables, \"6.2.2 Maximum Output Power\")\n",
    "    # display_processed_tables(desire_tables)\n",
    "    end_time = time.time()\n",
    "    print(f\"Time taken to find target table: {end_time - start_time:.2f} seconds\")\n",
    "   \n",
    "    \n",
    "    start_time = time.time()\n",
    "    processed_table = process_tables(desire_tables)\n",
    "    end_time = time.time()\n",
    "    print(f\"Time taken to process tables: {end_time - start_time:.2f} seconds\")\n",
    "    display_processed_tables(processed_table)\n",
    "    \n",
    "    \n",
    "    #append to clean_tables:\n",
    "    clean_tables.extend(processed_table)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Table Name</th>\n",
       "      <th>Testname</th>\n",
       "      <th>Band</th>\n",
       "      <th>ULCH</th>\n",
       "      <th>BW</th>\n",
       "      <th>MOD</th>\n",
       "      <th>RD</th>\n",
       "      <th>Limit Low</th>\n",
       "      <th>Limit High</th>\n",
       "      <th>Measured</th>\n",
       "      <th>Unit</th>\n",
       "      <th>Status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.2.2 Maximum Output Power</td>\n",
       "      <td>UE Maximum Output Power</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18607</td>\n",
       "      <td>1.4 MHz</td>\n",
       "      <td>QPSK</td>\n",
       "      <td>1 (RB_Pos:LOW)</td>\n",
       "      <td>18.80</td>\n",
       "      <td>25.70</td>\n",
       "      <td>22.71</td>\n",
       "      <td>dBm</td>\n",
       "      <td>Passed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.2.2 Maximum Output Power</td>\n",
       "      <td>UE Maximum Output Power</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18900</td>\n",
       "      <td>1.4 MHz</td>\n",
       "      <td>QPSK</td>\n",
       "      <td>1 (RB_Pos:LOW)</td>\n",
       "      <td>20.30</td>\n",
       "      <td>25.70</td>\n",
       "      <td>22.80</td>\n",
       "      <td>dBm</td>\n",
       "      <td>Passed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6.2.2 Maximum Output Power</td>\n",
       "      <td>UE Maximum Output Power</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18900</td>\n",
       "      <td>1.4 MHz</td>\n",
       "      <td>QPSK</td>\n",
       "      <td>5 (RB_Pos:LOW)</td>\n",
       "      <td>20.30</td>\n",
       "      <td>25.70</td>\n",
       "      <td>22.82</td>\n",
       "      <td>dBm</td>\n",
       "      <td>Passed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6.2.2 Maximum Output Power</td>\n",
       "      <td>UE Maximum Output Power</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19193</td>\n",
       "      <td>1.4 MHz</td>\n",
       "      <td>QPSK</td>\n",
       "      <td>1 (RB_Pos:LOW)</td>\n",
       "      <td>18.80</td>\n",
       "      <td>25.70</td>\n",
       "      <td>22.61</td>\n",
       "      <td>dBm</td>\n",
       "      <td>Passed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.2.2 Maximum Output Power</td>\n",
       "      <td>UE Maximum Output Power</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18615</td>\n",
       "      <td>3.0 MHz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Not Required for\\n3GPP default</td>\n",
       "      <td></td>\n",
       "      <td>Passed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.2.2 Maximum Output Power</td>\n",
       "      <td>UE Maximum Output Power</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18900</td>\n",
       "      <td>3.0 MHz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Not Required for\\n3GPP default</td>\n",
       "      <td></td>\n",
       "      <td>Passed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6.2.2 Maximum Output Power</td>\n",
       "      <td>UE Maximum Output Power</td>\n",
       "      <td>NaN</td>\n",
       "      <td>19185</td>\n",
       "      <td>3.0 MHz</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>Not Required for\\n3GPP default</td>\n",
       "      <td></td>\n",
       "      <td>Passed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6.2.2 Maximum Output Power</td>\n",
       "      <td>UE Maximum Output Power</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18625</td>\n",
       "      <td>5.0 MHz</td>\n",
       "      <td>QPSK</td>\n",
       "      <td>1 (RB_Pos:LOW)</td>\n",
       "      <td>18.80</td>\n",
       "      <td>25.70</td>\n",
       "      <td>22.80</td>\n",
       "      <td>dBm</td>\n",
       "      <td>Passed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6.2.2 Maximum Output Power</td>\n",
       "      <td>UE Maximum Output Power</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18625</td>\n",
       "      <td>5.0 MHz</td>\n",
       "      <td>QPSK</td>\n",
       "      <td>1 (RB_Pos:HIGH)</td>\n",
       "      <td>20.30</td>\n",
       "      <td>25.70</td>\n",
       "      <td>22.86</td>\n",
       "      <td>dBm</td>\n",
       "      <td>Passed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>6.2.2 Maximum Output Power</td>\n",
       "      <td>UE Maximum Output Power</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18900</td>\n",
       "      <td>5.0 MHz</td>\n",
       "      <td>QPSK</td>\n",
       "      <td>1 (RB_Pos:LOW)</td>\n",
       "      <td>20.30</td>\n",
       "      <td>25.70</td>\n",
       "      <td>22.84</td>\n",
       "      <td>dBm</td>\n",
       "      <td>Passed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Table Name                 Testname  Band   ULCH       BW   MOD               RD Limit Low Limit High                        Measured Unit  Status\n",
       "0  6.2.2 Maximum Output Power   UE Maximum Output Power   NaN  18607  1.4 MHz  QPSK   1 (RB_Pos:LOW)     18.80      25.70                           22.71  dBm  Passed\n",
       "1  6.2.2 Maximum Output Power   UE Maximum Output Power   NaN  18900  1.4 MHz  QPSK   1 (RB_Pos:LOW)     20.30      25.70                           22.80  dBm  Passed\n",
       "2  6.2.2 Maximum Output Power   UE Maximum Output Power   NaN  18900  1.4 MHz  QPSK   5 (RB_Pos:LOW)     20.30      25.70                           22.82  dBm  Passed\n",
       "3  6.2.2 Maximum Output Power   UE Maximum Output Power   NaN  19193  1.4 MHz  QPSK   1 (RB_Pos:LOW)     18.80      25.70                           22.61  dBm  Passed\n",
       "4  6.2.2 Maximum Output Power   UE Maximum Output Power   NaN  18615  3.0 MHz   NaN              NaN                       Not Required for\\n3GPP default       Passed\n",
       "5  6.2.2 Maximum Output Power   UE Maximum Output Power   NaN  18900  3.0 MHz   NaN              NaN                       Not Required for\\n3GPP default       Passed\n",
       "6  6.2.2 Maximum Output Power   UE Maximum Output Power   NaN  19185  3.0 MHz   NaN              NaN                       Not Required for\\n3GPP default       Passed\n",
       "7  6.2.2 Maximum Output Power   UE Maximum Output Power   NaN  18625  5.0 MHz  QPSK   1 (RB_Pos:LOW)     18.80      25.70                           22.80  dBm  Passed\n",
       "8  6.2.2 Maximum Output Power   UE Maximum Output Power   NaN  18625  5.0 MHz  QPSK  1 (RB_Pos:HIGH)     20.30      25.70                           22.86  dBm  Passed\n",
       "9  6.2.2 Maximum Output Power   UE Maximum Output Power   NaN  18900  5.0 MHz  QPSK   1 (RB_Pos:LOW)     20.30      25.70                           22.84  dBm  Passed"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(all_tables.head(10)) # Display the first few rows of the resulting dataframe\n",
    "\n",
    "# Concatenate all the dataframes in the list into a single dataframe\n",
    "all_tables = pd.concat(clean_tables, ignore_index=True)\n",
    "\n",
    "# Export the dataframe to an Excel file\n",
    "all_tables.to_excel(\"all_tables.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paul",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
