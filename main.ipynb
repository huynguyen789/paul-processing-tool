{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brainstorming:\n",
    "\n",
    "Problem: \n",
    "- User has large pdf file that has factory test data. \n",
    "- PDF file is too large: 22k pages\n",
    "- Data is in a table format\n",
    "- Data inside table is messy, need parsing/cleaning\n",
    "\n",
    "General solution:\n",
    "- Use python to parse pdf file\n",
    "- Get data from pdf file, parse data in desire format, export into excel file for visualization later\n",
    "\n",
    "Specific solution, pipeline:\n",
    "- Use camelot to parse tables from pdf files\n",
    "- Detect desire table, concat them.\n",
    "- Cleaning and organizing data into a nice/correct df:\n",
    "  - Check and drop extra column (Done)\n",
    "  - Detect and concat tables\n",
    "  - Seperate first cell into extra columns\n",
    "- Export df into excel file\n",
    "\n",
    "Design note:\n",
    "- Hardcoded: check all tables, if tables 7 columns: drop first one: all tables should have 6\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working code on all files:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-19 12:06:45.257 python[13494:1848459] +[CATransaction synchronize] called within transaction\n",
      "2023-07-19 12:06:45.261 python[13494:1848459] +[CATransaction synchronize] called within transaction\n"
     ]
    }
   ],
   "source": [
    "import camelot\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display\n",
    "import time\n",
    "import re\n",
    "from PyPDF2 import PdfReader\n",
    "import os \n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "def read_pdf(file_path):\n",
    "    # Read the PDF file using Camelot\n",
    "    print(f\"Start reading PDF file {file_path}...\")\n",
    "    tables = camelot.read_pdf(file_path, pages='all' )\n",
    "    # clean_tables(tables)\n",
    "    return tables\n",
    "\n",
    "def clean_table(table):\n",
    "    # print(f\"\\nStart cleaning table...\")\n",
    "    # Check if the table has more than 1 column\n",
    "    if table.shape[1] > 1:\n",
    "        # print(f'Table has {table.shape[1]} columns. Cleaning up...')\n",
    "        # # Drop the first column\n",
    "        table = table.drop(columns=0)\n",
    "        # Reset the column names\n",
    "        table.columns = range(table.shape[1])\n",
    "        # print(f'Table now has {table.shape[1]} columns.')\n",
    "    return table\n",
    "\n",
    "# Find target_table location in the PDF file: \n",
    "# return a list of page numbers\n",
    "def find_table_location(pdf_file, target_table):\n",
    "    print(f\"Searching for table: {target_table}\")\n",
    "    \n",
    "    reader = PdfReader(pdf_file)\n",
    "\n",
    "    pages_with_table = []\n",
    "\n",
    "    for page_num in range(len(reader.pages)):\n",
    "\n",
    "        print(f\"Searching page {page_num} out of {len(reader.pages)}\")\n",
    "        \n",
    "        page = reader.pages[page_num]\n",
    "        text = page.extract_text()\n",
    "        \n",
    "        if target_table in text:\n",
    "            print(f\"Found table on page {page_num}\")\n",
    "            pages_with_table.append(page_num)\n",
    "\n",
    "            # Check for multi-page\n",
    "            if page_num < len(reader.pages)-2:\n",
    "\n",
    "                # Get page_num+2 \n",
    "                next_page = reader.pages[page_num+2]  \n",
    "                next_text = next_page.extract_text()\n",
    "\n",
    "                if re.search(r'\\d+\\.\\d+\\.\\d+', next_text):\n",
    "                    pages_with_table.append(page_num + 2)\n",
    "\n",
    "            # Add extra page    \n",
    "            if pages_with_table:\n",
    "                pages_with_table.append(pages_with_table[-1] + 2)\n",
    "\n",
    "    # print(f\"Table pages found: {pages_with_table}\")\n",
    "                \n",
    "    return pages_with_table\n",
    "\n",
    "def convert_to_ranges(numbers):\n",
    "    \"\"\"\n",
    "    Convert a list of numbers into a list of ranges.\n",
    "    \"\"\"\n",
    "    ranges = []\n",
    "    \n",
    "    for i in range(0, len(numbers), 2):\n",
    "        \n",
    "        if i < len(numbers) - 1:\n",
    "            ranges.append(f\"{numbers[i]}-{numbers[i+1]}\")\n",
    "        else:\n",
    "            ranges.append(str(numbers[i]))\n",
    "\n",
    "    return ranges\n",
    "\n",
    "def display_tables(tables):\n",
    "    # Set the max column width to a high number (e.g., 1000) to display long contents\n",
    "    pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "    # Display all tables\n",
    "    print(f\"Displaying {len(tables)} tables:\")\n",
    "    for i, table in enumerate(tables):\n",
    "        table_number = i + 1\n",
    "        print(f\"\\nTable {table_number}\")\n",
    "        display(table.df)\n",
    "\n",
    "def display_processed_tables(tables):\n",
    "    # Set the max column width to a high number (e.g., 1000) to display long contents\n",
    "    pd.set_option('display.max_colwidth', 1000)\n",
    "\n",
    "    # Display all processed tables\n",
    "    print(f\"\\nDisplaying {len(tables)} tables:\")\n",
    "    for i, df in enumerate(tables):\n",
    "        print(f\"Table {i + 1}\")\n",
    "        display(df)\n",
    "\n",
    "def extract_band(tables):\n",
    "    print(f\"Extracting Band info...\")\n",
    "    for table in tables:\n",
    "        line2 = table.iloc[0,0]\n",
    "        if 'Band' in line2:\n",
    "            band_info = line2.split(' ')\n",
    "            for word in band_info:\n",
    "                if word.startswith('Band'):\n",
    "                    band_num = word[4:]  # Extract everything after \"Band\"\n",
    "                    table.insert(1, 'Band', band_num)\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"No 'Band' keyword in line: {line2}\")\n",
    "    return tables\n",
    "\n",
    "def extract_info(tables):\n",
    "    print(f\"Extracting Testname, ULCH, BW, MOD, RD info...\")\n",
    "    for i, table in enumerate(tables):\n",
    "        # Define patterns\n",
    "        testname_pattern = r\"(.*):@\"\n",
    "        ulch_pattern = r\"ULCH: (\\d+),\"\n",
    "        bw_pattern = r\"BW: ([\\d\\.]+ MHz)\"\n",
    "        mod_pattern = r\"UL_MOD_RB: ([^,]+),\"\n",
    "        rd_pattern = r\"UL_MOD_RB: [^,]+, (.*)\"\n",
    "\n",
    "        # Extract info\n",
    "        table['Testname'] = table.iloc[:,0].str.extract(testname_pattern)\n",
    "        table['ULCH'] = table.iloc[:,0].str.extract(ulch_pattern)\n",
    "        table['BW'] = table.iloc[:,0].str.extract(bw_pattern)\n",
    "        table['MOD'] = table.iloc[:,0].str.extract(mod_pattern)\n",
    "        table['RD'] = table.iloc[:,0].str.extract(rd_pattern)\n",
    "\n",
    "        # Drop the first column\n",
    "        table.drop(table.columns[0], axis=1, inplace=True)\n",
    "\n",
    "        # Reorganize the columns\n",
    "        print(f\"Reorganizing columns...\")\n",
    "        new_column_order = ['Testname', 'Band', 'ULCH', 'BW', 'MOD', 'RD', 'Limit Low', 'Limit High', 'Measured', 'Unit', 'Status']\n",
    "        table = table.reindex(columns=new_column_order)\n",
    "\n",
    "        # Replace the table in the list with the cleaned and reorganized table\n",
    "        tables[i] = table\n",
    "        \n",
    "    return tables    \n",
    "\n",
    "def find_target_table(tables, desired_name):\n",
    "    \n",
    "    current_table = None\n",
    "    desired_tables = []\n",
    "    \n",
    "    #Run through table_list\n",
    "    for table in tables:\n",
    "        first_cell = table.df.iloc[0,0].split('\\n')[0]\n",
    "\n",
    "        #If the cell in first row, first column has the desire format \"6.x.x.x\" AND the desire_name: Start a new table\n",
    "        if first_cell.startswith(\"6.\") and desired_name in first_cell:\n",
    "            # Found start of new desired table\n",
    "            \n",
    "            if current_table is None:\n",
    "                current_table = table\n",
    "            current_table = table.df.copy()\n",
    "            # print(f\"Found start of new desired table:\")   \n",
    "            # display(current_table)\n",
    "        elif first_cell.startswith(\"6.\") and desired_name not in first_cell: \n",
    "            #else: return the current table, and reset the current_table to None\n",
    "            # print(f\"\\nSkipping a none-desired table: {first_cell}\")\n",
    "            if current_table is not None:\n",
    "                # Save the current table\n",
    "                desired_tables.append(current_table)\n",
    "                #Reset, mark end of the desired table\n",
    "                current_table = None\n",
    "        \n",
    "        #if the table doesnt match the desire_format AND there is a current_table: concat this table into the current table:         \n",
    "        elif current_table is not None:\n",
    "            # Continuation of previous desired table\n",
    "            # print(f\"Found continuation of previous desired table: {table.df.iloc[0,0]}\")\n",
    "            # print(f\"Found continuous table, before cleaning:\")   \n",
    "            # display(table.df)\n",
    "            #Clean table before concat:\n",
    "            table.df = clean_table(table.df)\n",
    "            # print(f\"Continuous table after cleaning:\")   \n",
    "            # display(table.df)\n",
    "            current_table = pd.concat([current_table, table.df])\n",
    "\n",
    "            # print(f\"Table after concat:\")\n",
    "            # display(current_table)\n",
    "        \n",
    "            \n",
    "    return desired_tables\n",
    "\n",
    "def process_tables(tables):  \n",
    "\n",
    "    processed_tables = []\n",
    "    \n",
    "    for i, table in enumerate(tables):\n",
    "        # print(f\"\\nProcessing table {i+1}...\")\n",
    "        # print(f\"Table before processing:\")\n",
    "        # display(table)\n",
    "\n",
    "    \n",
    "        # Split the first cell of the first row and use it as column headers\n",
    "        first_row = table.iloc[0, 0]\n",
    "        headers = first_row.split('\\n')  \n",
    "        # print(f\"First row: {first_row}\")\n",
    "        # print(f\"Headers: {headers}\")\n",
    "        # headers.append(\"MissingHeader\")\n",
    "        table.columns = headers\n",
    "        \n",
    "        # Extract the table name from the first row\n",
    "        table_name = table.iloc[0, 0].split('\\n')[0]\n",
    "\n",
    "        \n",
    "        # Extract Band info from column 2\n",
    "        # print(f\"Extracting Band info...\")\n",
    "        line2 = table.iloc[1,0]\n",
    "        # print(f\"line2: {line2}\")\n",
    "        if 'Band' in line2:\n",
    "            band_info = line2.split(' ')\n",
    "            for word in band_info:\n",
    "                if word.startswith('Band'):\n",
    "                    band_num = word[4:]  # Extract everything after \"Band\"\n",
    "                    table.insert(1, 'Band', band_num)\n",
    "                    break\n",
    "            else:\n",
    "                print(f\"No 'Band' keyword in line: {line2}\")\n",
    "         \n",
    "\n",
    "        # print(\"Table after header:\")\n",
    "        # display(table)\n",
    "          \n",
    "        # # Check if the expected columns are in the table\n",
    "        # if 'Unit' not in table.columns:\n",
    "        #     print(f\"Table {i+1} doesn't have the expected structure. Skipping...\")\n",
    "        #     continue\n",
    "  \n",
    "        # Extract Testname, ULCH, BW, MOD, RD info\n",
    "        # print(f\"Extracting Testname, ULCH, BW, MOD, RD info...\")\n",
    "        # Define patterns\n",
    "        testname_pattern = r\"(.*):@\"\n",
    "        ulch_pattern = r\"ULCH: (\\d+),\"\n",
    "        bw_pattern = r\"BW: ([\\d\\.]+ MHz)\"\n",
    "        mod_pattern = r\"UL_MOD_RB: ([^,]+),\"\n",
    "        rd_pattern = r\"UL_MOD_RB: [^,]+, (.*)\"\n",
    "\n",
    "        # Extract info\n",
    "        table['Testname'] = table.iloc[:,0].str.extract(testname_pattern)\n",
    "        table['ULCH'] = table.iloc[:,0].str.extract(ulch_pattern)\n",
    "        table['BW'] = table.iloc[:,0].str.extract(bw_pattern)\n",
    "        table['MOD'] = table.iloc[:,0].str.extract(mod_pattern)\n",
    "        table['RD'] = table.iloc[:,0].str.extract(rd_pattern)\n",
    "        \n",
    "        # print(f\"Table before split Unit column:\")\n",
    "        # display(table)\n",
    "   \n",
    "        # Split 'Measured' and 'Unit' columns \n",
    "        # Create a temporary DataFrame for the split results\n",
    "        split_df = table['Unit'].str.split(expand=True)\n",
    "\n",
    "        # Assign the split results to 'Measured' and 'Unit' only where there are values\n",
    "        table.loc[split_df[0].notna(), 'Measured'] = split_df.loc[split_df[0].notna(), 0]\n",
    "        table.loc[split_df[1].notna(), 'Unit'] = split_df.loc[split_df[1].notna(), 1]\n",
    "\n",
    "        # Drop the first column\n",
    "        table.drop(table.columns[0], axis=1, inplace=True)\n",
    "\n",
    "        # Create a new column filled with the table name\n",
    "        table.insert(0, 'Table Name', table_name)\n",
    "        table = table.iloc[1:]\n",
    "        \n",
    "        # Reorganize the columns\n",
    "        # print(f\"Reorganizing columns...\")\n",
    "        new_column_order = ['Table Name', 'Testname', 'Band', 'ULCH', 'BW', 'MOD', 'RD', 'Limit Low', 'Limit High', 'Measured', 'Unit', 'Status']\n",
    "        table = table.reindex(columns=new_column_order)\n",
    "\n",
    "        # Drop rows with NaN values in Testname and Band columns\n",
    "        table = table.dropna(subset=['Testname'], how='all')\n",
    "\n",
    "\n",
    "        # Reset the index\n",
    "        table.reset_index(drop=True, inplace=True)\n",
    "        \n",
    "        #Append processed table to processed_tables:\n",
    "        processed_tables.append(table)\n",
    "        \n",
    "        \n",
    "    return processed_tables\n",
    "\n",
    "\n",
    "#UI TKINTER:\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "\n",
    "# Add a GUI file picker\n",
    "root = tk.Tk()\n",
    "root.withdraw()\n",
    "file_path = filedialog.askopenfilename()\n",
    "\n",
    "pdf_file = file_path\n",
    "# Extract name without extension\n",
    "file_name = os.path.splitext(os.path.basename(pdf_file))[0]\n",
    "\n",
    "# \"LTE_3GPP_v15_r8_FDD_FORD_All_TEMPS_TCU2_5_ROW_012023_5GSIM_AT_2023-06-28_16-27-57_188.pdf\"\n",
    "#  \"25C_DATA_Extract(1temperatureOnly).pdf\"\n",
    "# \"300_pages_extract.pdf\"\n",
    "\n",
    "target_table = \"6.2.2 Maximum Output Power\" \n",
    "# pdf_file = \"300_pages_extract.pdf\"\n",
    "# Extract name without extension\n",
    "file_name = os.path.splitext(os.path.basename(pdf_file))[0]\n",
    "\n",
    "\n",
    "# Find target table pages\n",
    "pages = find_table_location(pdf_file, target_table)\n",
    "\n",
    "# Format page numbers for camelot\n",
    "page_ranges = convert_to_ranges(pages)\n",
    "print(f\"Pages to read: {page_ranges}\")\n",
    "\n",
    "\n",
    "clean_tables = []\n",
    "total_ranges = len(page_ranges)\n",
    "\n",
    "\n",
    "for i, page_range in enumerate(page_ranges):\n",
    "    \n",
    "    print(f\"\\nProcessing page range {i+1}/{total_ranges}...\")\n",
    "    # Calculate the estimate time remaining based on the progress so far\n",
    "    progress = i / total_ranges\n",
    "    time_remaining = (total_ranges - i) * 9 / 60\n",
    "    print(f\"Estimated time remaining: {time_remaining:.2f} minutes\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    # Read all tables from pdf\n",
    "    long_tables = camelot.read_pdf(pdf_file, pages=page_range, backend=\"poppler\")\n",
    "    end_time = time.time()\n",
    "    print(f\"Time taken to load table: {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    desire_tables = find_target_table(long_tables, \"6.2.2 Maximum Output Power\")\n",
    "    # display_processed_tables(desire_tables)\n",
    "    end_time = time.time()\n",
    "    print(f\"Time taken to find target table: {end_time - start_time:.2f} seconds\")\n",
    "   \n",
    "    \n",
    "    start_time = time.time()\n",
    "    processed_table = process_tables(desire_tables)\n",
    "    end_time = time.time()\n",
    "    print(f\"Time taken to process tables: {end_time - start_time:.2f} seconds\")\n",
    "    # display_processed_tables(processed_table)\n",
    "    \n",
    "    \n",
    "    #append to clean_tables:\n",
    "    clean_tables.extend(processed_table)\n",
    "\n",
    "\n",
    "# Concatenate all the dataframes in the list into a single dataframe\n",
    "all_tables = pd.concat(clean_tables, ignore_index=True)\n",
    "display(all_tables.head(10)) # Display the first few rows of the resulting dataframe\n",
    "\n",
    "# Save to Excel using file name\n",
    "excel_file = f\"final_{file_name}.xlsx\" \n",
    "all_tables.to_excel(excel_file, index=False)\n",
    "\n",
    "#Auto open folder:\n",
    "import subprocess\n",
    "import platform\n",
    "folder_path = os.path.dirname(excel_file)\n",
    "\n",
    "if platform.system() == 'Windows':\n",
    "    os.startfile(folder_path)\n",
    "\n",
    "elif platform.system() == 'Darwin': \n",
    "    subprocess.Popen(['open', folder_path])\n",
    "\n",
    "else:\n",
    "    subprocess.Popen(['xdg-open', folder_path])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pipreqs ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"22kPDF.txt\", \"r\") as file:\n",
    "    page_ranges = file.read().splitlines()\n",
    "page_ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clean_tables = []\n",
    "total_ranges = len(page_ranges)\n",
    "for i, page_range in enumerate(page_ranges):\n",
    "    \n",
    "    print(f\"Processing page range {i+1}/{total_ranges}...\")\n",
    "    # Calculate the estimate time remaining based on the progress so far\n",
    "    progress = i / total_ranges\n",
    "    time_remaining = (total_ranges - i) * 9 / 60\n",
    "    print(f\"Estimated time remaining: {time_remaining:.2f} minutes\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    # Read all tables from pdf\n",
    "    long_tables = camelot.read_pdf(long_file, pages=page_range)\n",
    "    end_time = time.time()\n",
    "    print(f\"Time taken to load table: {end_time - start_time:.2f} seconds\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    desire_tables = find_target_table(long_tables, \"6.2.2 Maximum Output Power\")\n",
    "    # display_processed_tables(desire_tables)\n",
    "    end_time = time.time()\n",
    "    print(f\"Time taken to find target table: {end_time - start_time:.2f} seconds\")\n",
    "   \n",
    "    \n",
    "    start_time = time.time()\n",
    "    processed_table = process_tables(desire_tables)\n",
    "    end_time = time.time()\n",
    "    print(f\"Time taken to process tables: {end_time - start_time:.2f} seconds\")\n",
    "    display_processed_tables(processed_table)\n",
    "    \n",
    "    \n",
    "    #append to clean_tables:\n",
    "    clean_tables.extend(processed_table)\n",
    "    \n",
    "\n",
    "\n",
    "# Concatenate all the dataframes in the list into a single dataframe\n",
    "all_tables = pd.concat(clean_tables, ignore_index=True)\n",
    "display(all_tables.head(10)) # Display the first few rows of the resulting dataframe\n",
    "\n",
    "# Export the dataframe to an Excel file\n",
    "all_tables.to_excel(\"all_tables.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paul",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
